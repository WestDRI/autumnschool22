{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "## **Notebook 1: Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb079367-934a-4d8e-a5c3-ec5592ac69ae",
   "metadata": {},
   "source": [
    "**Introduction to Workshop Lab Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe05310-7998-4a4f-a660-f762f3359fab",
   "metadata": {},
   "source": [
    "Markdown cells contain plain text, and code cells contain interactive Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d06d849-08b0-4dfc-b76d-b60be5b172cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f80bd-c004-4d16-9f1b-974c971d2235",
   "metadata": {},
   "source": [
    "We can also run shell commands by prepending an exclamation mark to our code cells. Let's query for some basic information about our system. `nvidia-smi` is like `top` for NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74837c5-d1ed-4a7f-ba8c-a0d99c668093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 21 10:25:45 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    24W / 250W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd29b-e458-4c99-ab63-cb456524d5e3",
   "metadata": {},
   "source": [
    "Let's check out the connection topology of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03404cd-d681-46cd-b91a-1d1784eefcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\u001b[0m\n",
      "GPU0\t X \t0,2,4,6,8,10\t0\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c68ba-47c7-4f75-ba06-2244fc1ce936",
   "metadata": {},
   "source": [
    "And finally, let's check out the type of CPU we were allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ce24e3-22d0-4cd8-9691-132259ef2bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:          x86_64\n",
      "CPU op-mode(s):        32-bit, 64-bit\n",
      "Byte Order:            Little Endian\n",
      "CPU(s):                24\n",
      "On-line CPU(s) list:   0-23\n",
      "Thread(s) per core:    1\n",
      "Core(s) per socket:    12\n",
      "Socket(s):             2\n",
      "NUMA node(s):          2\n",
      "Vendor ID:             GenuineIntel\n",
      "CPU family:            6\n",
      "Model:                 79\n",
      "Model name:            Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz\n",
      "Stepping:              1\n",
      "CPU MHz:               2200.000\n",
      "CPU max MHz:           2200.0000\n",
      "CPU min MHz:           1200.0000\n",
      "BogoMIPS:              4400.02\n",
      "Virtualization:        VT-x\n",
      "L1d cache:             32K\n",
      "L1i cache:             32K\n",
      "L2 cache:              256K\n",
      "L3 cache:              30720K\n",
      "NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22\n",
      "NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23\n",
      "Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_pt ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts md_clear spec_ctrl intel_stibp flush_l1d\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a5376-6bd0-4d23-b61a-7a2954abab6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcc303-3c50-45c6-82f1-e7068e6d0665",
   "metadata": {},
   "source": [
    "NumPy is a widely used library for numerical computing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf823dc-3fef-431f-9abb-ee827c0cf838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.52 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "86.9 ms ± 122 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = np.random.randn(size, size)\n",
    "\n",
    "%timeit -n 5 Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dcb8c-7141-46f3-bd08-ea07261e9e65",
   "metadata": {},
   "source": [
    "CuPy uses a NumPy-like interface. Porting a Numpy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d196d0-c618-4805-863a-3b57e873d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.86 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = cp.random.randn(size, size)\n",
    "\n",
    "Q, R = cp.linalg.qr(A)\n",
    "%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954a9de-3519-400e-8a44-d59668bd26f2",
   "metadata": {},
   "source": [
    "We already see a substantial speedup with no real code changes! \n",
    "\n",
    "Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n",
    "\n",
    "NumPy is typically used to perform computations on _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host memory, the contents of a `cupy.ndarray` persistent in GPU memory. CuPy provides several helper functions to convert between Cupy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca4b14c-fe6e-4ba7-82fc-0c50219e1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_cpu is a <class 'numpy.ndarray'>\n",
      "With initial values:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "A_gpu is a <class 'cupy.ndarray'>\n",
      "Squared values:\n",
      " [[ 1  4  9]\n",
      " [16 25 36]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the data on the host\n",
    "A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "print(\"A_cpu is a\", type(A_cpu))\n",
    "print(\"With initial values:\\n\", A_cpu)\n",
    "\n",
    "#Copy data, host to device\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "print(\"A_gpu is a\", type(A_gpu))\n",
    "\n",
    "#Square the data on the device\n",
    "A_gpu = cp.square(A_gpu)\n",
    "\n",
    "#Copy data, device to host\n",
    "A_cpu = cp.asnumpy(A_gpu)\n",
    "\n",
    "print(\"Squared values:\\n\", A_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2",
   "metadata": {},
   "source": [
    "Note that NumPy and CuPy ndarrys are not implicitly convertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7eacf74-5f07-42b0-9b6f-8af7a302d25a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19289/1477215872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_arg\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "cp.square(A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ba905-52c8-44af-9bea-461cd63e13c4",
   "metadata": {},
   "source": [
    "CuPy is useful for programming multi-GPU nodes as well. We can orchestrate computation, data movement, and other low-level CUDA operations with functions in the `cupy.cuda` namespace. The following cell is shown for demonstration purposes only. It will not execute on our system as we've requested 1-GPU per student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d59f70-83dc-444c-897d-ee5f9c94971a",
   "metadata": {},
   "source": [
    "```\n",
    "#Initialize array on GPU 1\n",
    "with cp.cuda.Device(1):\n",
    "    A_gpu_1 = cp.array([[1, 2, 3], [4, 5, 6]], cp.int32)\n",
    "\n",
    "#Copy array from GPU 1 to GPU 0\n",
    "A_gpu_0 = cp.asarray(A_gpu_1)\n",
    "\n",
    "print(A_gpu_0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84591b-2329-44a8-a266-baa30866086b",
   "metadata": {},
   "source": [
    "The GPU is a powerhouse of parallel computing performance, and can process math operations much more quickly than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices. Notice the similarity of the two versions of the code (NumPy and CuPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c64595b-a9b7-43c4-a5ed-87b579aca4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Elapsed wall clock time for numpy = 2.75831 seconds.\n",
      "\n",
      "\n",
      "    Elapsed wall clock time for cupy = 0.037026 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4096\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C_cpu = np.matmul(A_cpu,B_cpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_cpu\n",
    "del B_cpu\n",
    "del C_cpu\n",
    "\n",
    "\n",
    "\n",
    "A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead\n",
    "start_time = perf_counter( )\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_gpu\n",
    "del B_gpu\n",
    "del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9b000-e8fa-4f67-a219-4640aea54abf",
   "metadata": {},
   "source": [
    "The GPU's strenghts in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel Overhead**\n",
    "\n",
    "CuPy compiles kernel codes on-the-fly using JIT compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1a0d2b-da82-4e46-999c-55bbad4c0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2637\n",
      "0.0045\n",
      "0.0044\n",
      "0.0044\n",
      "0.0044\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "size = 512\n",
    "for _ in range(5):\n",
    "    A = cp.random.randn(size, size).astype(np.float32)\n",
    "    t1 = time.time()\n",
    "    cp.linalg.det(A)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    t2 = time.time()\n",
    "    print('%.4f' % (t2 - t1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d8d9b-e4b2-4376-a441-b6bbf9a78af8",
   "metadata": {},
   "source": [
    "You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n",
    "\n",
    "In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2061b6b0-169f-4e64-8556-b3fd86b97dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Matrix size: 64 x 64 \n",
      "numpy 0.000343\n",
      "cupy 0.000636\n",
      "\n",
      "Input Matrix size: 128 x 128 \n",
      "numpy 0.001521\n",
      "cupy 0.001880\n",
      "\n",
      "Input Matrix size: 256 x 256 \n",
      "numpy 0.006202\n",
      "cupy 0.005085\n",
      "\n",
      "Input Matrix size: 512 x 512 \n",
      "numpy 0.036518\n",
      "cupy 0.010514\n",
      "\n",
      "Input Matrix size: 1024 x 1024 \n",
      "numpy 0.231632\n",
      "cupy 0.023783\n",
      "\n",
      "Input Matrix size: 2048 x 2048 \n",
      "numpy 1.750009\n",
      "cupy 0.065319\n"
     ]
    }
   ],
   "source": [
    "for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n",
    "    for xp in [np, cp]:\n",
    "        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(xp.float32)\n",
    "        xp.linalg.qr(A)#Exclude potential one-time JIT overhead\n",
    "        t1 = time.time()\n",
    "        xp.linalg.qr(A)\n",
    "        cp.cuda.Device().synchronize()\n",
    "        t2 = time.time()\n",
    "        print(xp.__name__, '%f' % (t2 - t1))\n",
    "        del A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335660-dfa6-4657-9489-7d31bb491d93",
   "metadata": {},
   "source": [
    "It's clear that increasing the problem size can help amoritize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689b1c62-62f6-4da9-a3de-3e5afd5beda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 97.69 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "737 µs ± 1.68 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 119.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "396 µs ± 914 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "@cp.fuse\n",
    "def fused_squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "x = cp.arange(size)\n",
    "y = cp.arange(size)[::-1]\n",
    "\n",
    "%timeit -n 10 squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "%timeit -n 10 fused_squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "\n",
    "del x\n",
    "del y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e7785-3b82-4e6e-9742-76b1caed35c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Data Movement Overhead**\n",
    "\n",
    "Try to minimize data movement to or from the GPU. The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real inbalance we have to deal with for now.\n",
    "Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later.\n",
    "\n",
    "Notice again the similarity of the two parts of the code (NumPy and CuPy). They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebb0179-8137-4827-8eb2-2c960967593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "numpy = 2.90858 seconds\n",
      "cupy = 0.0344925 seconds\n",
      "Speedup = 84.32\n",
      "\n",
      "Iteration  1\n",
      "numpy = 2.96167 seconds\n",
      "cupy = 0.0225139 seconds\n",
      "Speedup = 131.55\n",
      "\n",
      "Iteration  2\n",
      "numpy = 2.91601 seconds\n",
      "cupy = 0.0224895 seconds\n",
      "Speedup = 129.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    print('numpy = %g seconds' % cpu_time )\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "    A_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    B_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea831-f9e3-41c1-888d-8fed77d9fd1f",
   "metadata": {},
   "source": [
    "But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`. \n",
    "\n",
    "Modify the following cell to initialize the ndarray data with Numpy. \n",
    "\n",
    "How does the speedup change after the additional cost of data movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be63e60-fb09-4936-886d-7e91aebacd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19289/1325220631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mA_cpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mB_cpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "#>>>Insert CuPy code here\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc692d-137a-44ab-8743-3a2fa3ff60bc",
   "metadata": {},
   "source": [
    "Click the `...` below to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a42a53-e98c-4434-95ab-2114ee4d6dcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    \n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    \n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e8fff-90ec-4af3-b453-d1a97d83c86f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Managing GPU Memory**\n",
    "\n",
    "Modern datacenter GPUs have as much as 80GB of high-bandwidth memory on a single accelerator. But in general, the host system memory will have a larger capacity. We need to be conscious of GPU memory limitations when transfering data from the host. We can query the amount of free and total memory with nvidia-smi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7af7910-1168-4bc6-bc9a-f6e887f2bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.free [MiB], memory.total [MiB]\n",
      "9573 MiB, 12288 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18ee84-662a-4611-b142-0844ff46bb0b",
   "metadata": {},
   "source": [
    "Or natively with CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbbc8213-3cad-4d2b-a6dc-c345db8c1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (free, total) memory in bytes:\n",
      "(10038345728, 12790988800)\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038935eb-bce2-406e-9a4e-a07b23764c08",
   "metadata": {},
   "source": [
    "Let's clear all GPU memory for good measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2e4d096-f36f-43eb-bdc7-7af50327342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (free, total) memory in bytes:\n",
      "(11527323648, 12790988800)\n"
     ]
    }
   ],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659485d-679c-49f3-81ec-4be29295fe9d",
   "metadata": {},
   "source": [
    "What happens if we try to allocate too much space on the GPU? In the following example, arrays A and B are 8GB each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e011c0b-1b76-4320-bd09-9769379c543f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 8,589,934,592 bytes (allocated so far: 9,394,129,408 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19289/870325487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/cupy/_creation/basic.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Out of memory allocating 8,589,934,592 bytes (allocated so far: 9,394,129,408 bytes)."
     ]
    }
   ],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8d6e1-240c-4883-8462-4e1dab976ca5",
   "metadata": {},
   "source": [
    "One possible solution is to switch over to unified memory. With unified memory, the CUDA runtime will migrate data between the CPU and GPU _on demand_. Data migrations are triggered by page faults, so we may be leaving some performance on the table by using unified memory instead of managing memory explicitly. But it's an extremely convenient feature for making GPUs easier to program. We can enable Unified Memory in CuPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c121f48-b01e-4b17-a83c-3210472ae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a memory pool instance with malloc_managed allocator\n",
    "pool = cp.cuda.MemoryPool(cp.cuda.malloc_managed)\n",
    "cp.cuda.set_allocator(pool.malloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0a2ab-da68-49df-b712-a7cbba943c3b",
   "metadata": {},
   "source": [
    "Let's try that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2ad44e-dcc0-475f-a085-b770dd105ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58382e8-2e73-488c-b318-6974deb80eba",
   "metadata": {},
   "source": [
    "We can certainly perform computations on these new arrays. Performance will take a hit as the GPU swaps pages in-and-out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b8daa-ef79-42f6-ac3a-7213f7fbb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.add(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076ac4-4ec3-4913-a891-87f029760155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4066fd-d3f9-432c-9f41-73276dd3af94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
